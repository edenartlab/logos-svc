This is the full documentation to the Eden project, about the Eden creation tool, concept trainer, and SDK.

# Introduction

Eden's [mission](/docs/overview/mission) is to make expressive generative AI tools which are accessible to creators, and to foster a fun, social, and collaborative ecosystem around AI art.

Our flagship product is a social network that enables creators to generate, share, and remix art, train custom models, and deploy interactive agents and chatbots. We also provide a [convenient SDK](/docs/sdk/quickstart) that allows developers to build their own apps on top of Eden.

Eden was founded by a team of artists, creative technologists, and longtime machine learners. We are [committed to open source](https://github.com/abraham-ai), and a highly community-driven project, with numerous ways for creators and developers to plug in and contribute.

# Overview

These docs are divided into two sections. 

The first section provides a set of guides for working with Eden as a creator, including:

- [How to use the creation tool](/docs/guides/creation) to make creations in the form of images, videos, and text.
- [How to train custom models or "concepts"](/docs/guides/concepts) in order to tailor the generators towards a specific domain, style, genre, or character.
- [How to deploy your own characters](/docs/guides/characters), autonomous creative agents with whom you can chat with, assign artistic tasks to, or challenge other characters to debates or games.

The second section is a technical reference containing:

- [An introduction to the Eden API and SDK](/docs/sdk/quickstart), a JavaScript library that allows you to interact with Eden programatically and build custom applications with it.
- [A guide for how to host custom models or code](/docs/sdk/generators) on Eden.

# Mission

With the rise of generative models that produce realistic text, images, video, and sound, a new artform and industry has formed atop a relatively small cache of open-source models, datasets, code, and compute resources. Many tools, services, and APIs have emerged to support users of these resources and meet the needs of this ecosystem.

Despite the prevalence of open-source models and tooling, much effort is duplicated reinventing solutions to common problems such as data acquisition, training, customizing, deployment, and serving models in a reliable and efficient way. Most of these services are closed or proprietary, creating platform risk for users and application developers, and limiting innovation from users who are unable to access or modify the underlying code or data.

Eden improves this by fostering a commons for the generative AI space.

Our core principles are:

- **Open source and open access**. We believe in the power of open source software to foster innovation, collaboration, and democratization in AI.
- **Creator-centric**. We believe that creators should in charge and in control of their own data and artistic outputs.
- **Interoperable and composable**. We strive to make our tools maximally expandable, adaptable, and interoperable with other platforms and tools.

Problems we are trying to solve:

* **Curation and discovery**: in the age of infinite content, how can we help users create or find what they are looking for?
* **Cold-start problem**: how can new artistic projects get off the ground without a large corpus of data or a large audience?
* **Generative AI platform risk**: how can we ensure that users are not locked into a single platform or service?

# Manna

Manna is the internal currency of Eden. Creators must expend Manna to take actions on Eden, including to [make creations](/docs/guides/creation).

The [creation tool](https://app.eden.art/create) is priced according to the following:
* Each image produced by /create and /remix costs 1 manna.
* Each vieeo produced by /interpolate and /real2real costs 1 manna per frame.

## How to get manna

New users receive 1000 free manna upon sign-up.

To top up, manna may be purchased as well.

The Eden team actively gives out free manna to community members who actively contribute to Eden in some way. To learn more or propose collaborations, get in touch with us [on Discord](https://discord.gg/4dSYwDT).

## Overview
If you havent already, go to the **[Eden App](https://app.eden.art/)** and login with your email to get started!
Before diving into each of Edens endpoints separately, lets do a quick overview of what each one does:

#### Image endpoints:
- **Create** is our *'text-to-image'* pipeline, allowing you to create images from prompts using [SDXL](https://stability.ai/stablediffusion) (StableDiffusion XL)
- **ControlNet** lets you 'style-transfer' a guidance image using prompts
- **Blend** takes two images and creates a blend of them.
- **Upscale** upscales a single image to a higher resolution.
- **Remix** takes a single image and creates variations of it (prompts are optional).

#### Video endpoints:
- **Interpolate** is an extension of create where you enter multiple prompts and get a interpolation video back that morphs through those prompts
- **Real2Real** is like Interpolate, but instead of prompts, it start from images only. You upload a sequence of images and real2real will generate a smooth video morph between your images!

Most of these endpoints are fairly easy to use with just the default settings, but getting good results with AI requires some of understanding about what goes on under the hood, so let's dive in!

## 1. /create

**[Create](https://app.eden.art/create/creations)** is our *text-to-image* endpoint, powered by StableDiffusion XL. Set your desired image resolution, enter your prompt and hit create, simple as that!

If you’re the first person to trigger a creation job in a while, it is possible that our backend will spin up a new gpu-box for you, which might take a few minutes. Once a gpu is up and running, image creations should take around 5-10 seconds with default settings.

### Optional settings
Every one of our endpoints has a dropdown *'Show optional settings'* that offers a ton of additional features. Lets go over them:

- ***'Width'*** and ***'Height'*** set the amount of pixels and aspect ratio of your creation. Note that if you are using init images or doing real2real, the generator will automatically adopt the aspect ratio of your inputs and distribute the total amount of pixels (width x heigth) over that aspect ratio.
- ***'Upscale Factor'*** wil upscale the resolution of your generated image by the given factor after generating it with SDXL. If you want very HD images, upscaling is generally better than simply rendering at higher starting resolutions (width and height). This is because the model is trained for a specific resolution and going too far beyond that can create repeating artifacts in the image, but feel free to experiment here!
- ***'concept'*** and ***'concept-scale'*** allow you to activate a trained concept in your creation, one of the most powerful features on Eden. See our **[concept-trainer guide](https://docs.eden.art/docs/guides/concepts)** for all the details!
- ***'ControlNet or Init image'*** let’s you upload an image that the model will use as a color and shape template to start drawing from. This allows much more control over what the final image should look like.
- The ***‘Init image strength’*** controls how heavily this init image influences the final creation. SDXL is very sensitive to init_images so you usually want to set low values, a good first value to try is 0.2 Values above 0.5 will look almost identical to your init image.
- ***'samples'*** allows you to generate multiple variations with a single job.
- ***'negative prompt'*** allows you to specify what you DONT want to see in the image. Usually keeping this at default is fine, but feel free to experiment!
- ***'guidance scale'*** how strongly the prompt drives the creation. Higer values usually result in more saturated images.
- ***'sampler'*** the diffusion sampler to use, see [here](https://huggingface.co/docs/diffusers/v0.20.0/en/api/schedulers/overview)
- ***'steps'*** how many denoising steps to use. Higher values will be slower but sometimes produce more details. Strong diminishing returns past 40 steps.
- ***'seed'*** random seed for reproducibility. Fixing the seed can make it easier to determine the precise effect of a certain parameter while keeping everything else fixed.

## 2. /controlnet
Controlnet allows you to adopt the shape / contours of a control image into your creation, but still apply the style and colors with a text prompt.
The best way to understand controlnet is to just show it:

#### Step 1: Upload your control image:
Input: the original logo for Abraham, our autonomous digital artist

#### Step 2: Pick your controlnet type ("canny-edge" or "depth" currently supported)
This will cause different kinds of controlnet conditioning:
  - canny-edge will try to produce a creation that has the same canny-edge map as your control image
  - depth will try to produce a creation that has the same depth map as your control image
  - luminance will try to mimic the bright and dark regions in your control image, it is probably the best controlnet model.
Experiment!

#### Step 3: Set the init image strength
This value controls how strongly the control image affects the creation.  
Usually values between and 0.4-0.8 are good starting points.

## 3. /interpolate
Interpolate lets you create smooth interpolation video’s by entering a sequence of prompts. This allows you to create simple, linear video narratives and is fully compatible with **[custom concepts](https://docs.eden.art/docs/guides/concepts)**. Here’s a simple videoloop between the following prompts:
    - "a photo of a single lone sprout grows in a barren desert, the horizon is visible in the background, low angle 8k HD nature photo"
    - "a photo of a lone sappling growing in a field of mud, realistic water colour"
    - "a photo of a huge, green tree in a forest, the tree is covered in moss, 8k HD nature photo"
    - "a photo of an old, crumbled Tree of life, intricate wood folds, 8K professional nature photography, HDR"

### Lerp + ControlNet:

Just like with /Create, you can use an Init image combined with ControlNet "canny-edge" to create an interpolation video guided by a control image:
<iframe width="500" height="500" src="https://storage.googleapis.com/public-assets-xander/A_workbox/eden_docs/eden_lerp.mp4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

The above was created with the Abraham logo as init image and a controlnet image strength of 0.65

## 4. /real2real

**Real2Real** is an algorithm we’re pretty proud of. It essentially does the same as lerp, except that here, the input is not a sequence of prompts, but a sequence of arbitrary images. The algorithm will then create a smoothed video interpolation morphing between those real input images, no prompt engineering required.

Real2Real accepts ANY input image, so you can eg import images from MidJourney, use photographs, sketches, video frames, …

Below is an example of a Real2Real morphing between the following input images:

Note that while Real2Real accepts litterally any input image, the quality of the interpolation will depend on how well the generative model can represent the input images. Eg StableDiffusion was not particularly trained on faces and so Real2Real tends to give underwhelming results on face interpolations.

Like our other endpoints, Real2Real has a few customization parameters that can dramatically affect the results from this algorithm:

- ***'FILM iterations'***: when set to 1, this will post-process the video frames using FILM, dramatically improving the smoothness of the video (and doubling the number of frames).
- ***'Init image min strength'***: the minimum strength of the init_imgs during the interpolation. This parameter has a significant effect on the result: low values (eg 0.0–0.20) will result in interpolations that have a longer “visual path length”, ie: more things are changing and moving: the video contains more information at the cost of less smoothness / more jitter. Higher values (eg 0.20–0.40) will seem to change more slowly and carry less visual information, but will also be more stable and smoother.
→ Experiment and see what works best for you!
- ***'Init image max strength'***: the maximum strength of the init_imgs during the interpolation. Setting this to 1.0 will exactly reproduce the init_imgs at the keyframe positions in the interpolation at the cost of a brief flicker (due to not being encoded+decoded by VQGAN). Setting this to lower values (eg 0.70–0.90) will give the model some freedom to ‘hallucinate’ around the init_img, often creating smoother transitions. Recommended values are 0.90–0.97, experiment!

## 5. /remix

Remix does exactly what you think it does: it takes an input image and creates a variation of it. Internally, remix will try to construct a prompt that matches your image and use it to create variations of your image with.

The most important parameter here is:

- Init image strength: controls how much influence the init image has over the final result. Setting this to 0.0 will produce a remix that is entirely based on the ‘guessed prompt’ for the image and not influenced at all by the actual colors / shape of the input image. This could produce more creative images but will diverge more from the original.

## 6. /blend
Blend takes two input images and will produce a blended / mixed version of them as output.

## 7. /upscale
Upscale takes a single input image and will produce an upscaled version of it. The parameters are:
- ***'Init image strength'*** how strongly to use the original image. Lower values give the upscaler more freedom to create new details, often leading to a sharper final image, but will also deviate more from the original. Recommended values are 0.3-0.7


### **Summary**
1. Train a new concept by uploading images to the [concept trainer](https://app.eden.art/create/concepts) and picking a training mode.
2. Wait for training to finish (takes 5-10 mins)
3. Go to the [creation tool](https://app.eden.art/create/creations) (/create, /interpolate or /real2real)
4. Select your concept from the concept dropdown menu
5. Trigger the concept by adding <concept\> in your prompt text (not needed for styles & real2real):  
eg ***"a photo of <concept\> climbing a mountain"***
6. **If things dont look good, instead of messing with the settings, try changing your training images: they're the most important input variable!**

## Introduction
**Concepts** are custom characters, objects, styles, or specific people that are not part of the base generative model's (SDXL) knowledge, but that can be trained into the model by showing it a few examples of your concept. Once trained, you can naturally compose with concepts in your prompts just like you'd normally do with things the model knows already, eg a person named 'Barack Obama' or a style like 'cubism'.

Concepts are first trained by uploading example images to the [concept trainer](https://app.eden.art/create/concepts). After training finishes (this takes about 5 mins), the concept becomes available to use in the main creation tool and is compatible with single image creates, interpolations and real2real. Note that a concept has to be:
- activated in the creation by selecting the corresponding name from the concept dropdown menu
- triggered by using <concept\> to refer to it in the prompt text.

Concepts are a highly versatile and powerful creation tool. They can be used to capture a specific person's face or likeness, an animated character, or a complex object. They can also be more abstract, referring to a particular artistic style or genre.

## Training

The concept trainer is available at [https://app.eden.art/create/concepts](https://app.eden.art/create/concepts) and is a rework of the great LORA trainer created by [@cloneofsimo](https://twitter.com/cloneofsimo) over [here](https://github.com/replicate/cog-sdxl).

To train a good concept you need just a few (3-10 images is fine), but really good training images. Really good in this context means:
- good resolution (at least 768x768 pixels is recommended)
- diverse (it's better to have 5 very diverse images than 20 almost identical ones)
- well cropped, clearly showing the concept you're trying to learn

The training images are the most important part of concept training, if things dont look good, instead of changing the settings, just try a different (sub-) set of training images!

## Generating with concepts:

Once a concept has been trained, here's how to use it:
1. Select your trained concept from the concept dropdown menu in the creation tool:

2. If the concept was trained with "style" mode you can prompt as normal. If the concept was trained with "face" or "concept" mode, you have to trigger your concept/face in the prompt. There are two options to do this:
   - You can either trigger your concept by referring to it as <concept\> in your prompt text, eg  
   ***"a photo of <concept\> climbing a mountain"***
   - Or you can use the actual name of your trained concept. Eg if my concept name was "Banny" I could prompt-trigger it like so:  
   ***"a photo of <Banny\> climbing a mountain"***

3. When generating you can adjust the concept scale, which will control how strongly the concept is being used in the generation. 0.8 is usually perfect (1.0 usually doesn't work so well!), but in some cases, when the concept is slightly overfit, you can try to lower this value to get more promptability.

Note: all the example images in this post were generated with the default trainer & generation settings!

## Examples
### Example: face-mode

Generative models like Stable Diffusion are great at generating realistic faces. However, the model obviously doesn't know what everyone looks like (unless you are very famous). To get around this, we can train a concept to learn a specific person's face.
When training "face" concepts it is recommended to disable the random left/right flipping of training images (see more details below under **"advanced parameters"**).

For example, the training samples below are of [Xander](https://twitter.com/xsteenbrugge).

After training, we can use the concept <Xander\> in a prompt to generate realistic and figurative pictures:
- <Xander\> as a character in a noir graphic novel
- <Xander\> action figure
- <Xander\> as a knight in shining armour
- <Xander\> as the Mona Lisa
- etc ...

Faces are a popular and easy use case. It is possible to learn a face accurately from a single image, although two or three images are usually recommended to provide a bit of additional diversity.

### Example: concept-mode

**Concepts** can also be used to model consistent objects or characters. The above images are professional renders of the character for our Kojii project. This is a good example of a great training set since it contains: a single, consistent character with subtle variations in pose and appearance between every image. After training a new concept with name "kojii" with mode 'concept' and default settings, we get a fully promptable Kojii character, eg (see top image row):
- a photo of <kojii\> surfing a wave
- <kojii\> in a snowglobe
- a low-poly artwork of <kojii\>
- a photo of <kojii\> climbing mount Everest, alpinism
- etc ...

### Example: style-mode

Concepts can also be used to model artistic styles. For example, the following training samples below are artworks originally created by [VJ Suave](https://vjsuave.com/).

You can then train a concept using the "style" mode, and generate with it in /create. For style concepts, you dont even have to trigger the concept in any way, just prompt like you normally would.
The following are samples are all generated from the trained Suave concept (using default settings for both the trainer and creations):

## Training parameters

### Required parameters:

* **Concept name**: The name of the concept. This can be used to refer to the concept in prompts. Names are not required to be unique and can be reused.
* **Training images**: The images to use for training. You can upload image files (jpg, png, or webm), or you can upload zip files containing multiple images. You may upload up to 10 files, and each file must be below 100MB. From our experiments, the concept training actually works best if you dont have too many images. We recommend using 3-10 high quality and diverse images.
* **Training mode**: There are three available modes: concept, face, and style. They refer to trainer templates that are optimized for these three categories. Faces refer to human faces, concepts may refer to objects, characters, or other "things," while styles refer to the abstract style characteristics common to all the training images. Select the one that best matches your training set.

The trainer is designed to handle most cases well with the default settings, particularly in the case of concepts. Some concepts and  styles are more challenging to capture well and may require some trial and error adjusting the optional settings to achieve the right balance of diversity, accuracy, and promptability. To give some intuitions about how the advanced settings may affect the results, we describe them below. 

However keep in mind that **the most important input parameter are the training images themselves**: if things dont look good, instead of spending hours fiddling with the advanced parameters, we highly recommend to first try training again with a different subset of your images (using default parameters).

### Advanced parameters:

* **Number of training steps**: This refers to how long to finetune the model with your dataset. More steps should lead to fitting your concept more accurately, but too much training may "overfit" your training data, leading the base model to "forget" much of its prior knowledge (prompting wont work well anymore) and produce visual artifacts.
* **To randomly flip training imgs left/right**: This setting doubles the number of training samples by randomly flipping each image left/right. This should generally be on, unless the object you want to learn has a specific horizontal orientation which should not appear mirrored (for example text (LOGO's) or faces).
* **Learning rate for the LORA matrices**: The learning rate for the LORA matrices that adjust the inner mechanics of the generative model to be able to draw your concept. Higher values lead to 'more/faster learning' usually leading to better likeness at the cost of less promptability. So if the creations dont look enough like your training images --> try increasing this value, if your images dont listen to your prompts --> try decreasing this value.
* **Learning rate for textual inversion phase** : Textual inversion refers to the part of the training process which learns a new dictionary token that represents your concept. So in the same way that StableDiffusion knows what a "table" is and can draw tables in many different forms and contexts, it will learn a new token that represents your concept.
* **LORA rank** : Higher values create more 'capacity' for the model to learn and can be more succesful for complex objects or styles, but are also more likely to overfit on small image sets. The default value of 4 is recommended for most cases.
* **trigger text** : Optional: a few words that describe the concept to be learned (e.g "man with mustache" or "cartoon of a yellow superhero"). Giving a trigger text can sometimes help the model to understand what it is you're trying to learn and tries to leverage prior knowledge available in the model. When left empty, the trigger text will be automatically generated (recommended).
* **Resolution** : Image resolution used for training. If your training resolution is much lower than the resolution you create with, the concept will appear smaller inside your larger image and will often have repeating artefacts like multiple noses or copies of the same face. Training at lower resolutions (eg 768) can be useful if you want to learn a face but want to prompt it in a setting where the face is only a small part of the total image. Using init_images with rough shape composition can be very helpful in this scenario.
* **Batch size** : Training batch size (number of images to look at simultaneously during training). Increasing this may lead to more stable learning, however all the above values have been finetuned for batch_size = 2. Adjust at your own risk!

# Tips & trics

:::tip
- the advanced settings are pretty well optimized and should work well for most cases.
- When things dont look good: try changing your training images before adjusting the settings!
:::tip

:::warning
- When uploading face images, it's usually a good idea to crop the images so the face fills a large fraction of the total image.
- We're used to "more data is always better", but for concept training this usually isn't true: 5 diverse, HD images are usually better than 20 low-quality or similar images.
:::warning


## How to use the SDK

:::info
API keys are currently in beta. If you'd like to use the SDK, please reach out to the devs on [Discord](https://discord.com/invite/4dSYwDT).
:::info

The Eden SDK is a JavaScript library for interacting with the Eden API. The SDK allows you to make creation requests programatically and integrate Eden-facing widgets into your own applications. It is available as an npm package, with a commonjs version and Python SDK also planned for the near future.

## Get API credentials

To get an API key, please message one of the devs in [the Discord](https://discord.com/invite/4dSYwDT) and ask for one.

## Installation

You can install the SDK with npm, yarn, or pnpm:

```bash
npm install @edenlabs/eden-sdk
```

## Make a creation

A full list of generators and their config parameters can be found in the [creation tool](https://app.eden.art/create).

All requests to Eden go through the `EdenClient` class. To make a task request, target a specific generator (e.g. "create") with a configuration object. For example:

```js
import {EdenClient} from "@edenlabs/eden-sdk";

const eden = new EdenClient({
  apiKey: "YOUR_EDEN_API_KEY",
  apiSecret: "YOUR_EDEN_API_SECRET",
});

const config = {
  text_input: "An apple tree in a field",
};

const taskResult = await eden.tasks.create({
  generatorName: "create", 
  config: config
});
```

The `create` method is asynchronous and will immediately return a `taskResult` object with an ID for that task (or an error message). If you want to wait for the task to complete, you can poll the task until it is done, like so:

```js
const pollForTask = async function(pollingInterval, taskId) {
  let finished = false;
  while (!finished) {
    const taskResult = await eden.tasks.get({taskId: taskId});
    if (taskResult.task.status == "failed") {
      throw new Error('Failed')
    }
    else if (taskResult.task.status == "completed") {
      finished = true;
      const url = taskResult.task.creation.uri;
      return url;
    }
    await new Promise(resolve => setTimeout(resolve, pollingInterval))
  }
}

const result = await pollForTask(5000, taskResult.taskId);
```

## Manna

To get your user's [Manna](/docs/overview/manna) balance, use:

```js
const manna = await eden.manna.balance();
console.log(manna);
```

:::warning
There is currently no way to retrieve the cost in Manna of a specific config or job requests. This is a high priority feature.
:::warning

