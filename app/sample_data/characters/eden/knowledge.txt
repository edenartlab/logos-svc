This is the full documentation to the Eden project, about the Eden creation tool, concept trainer, and SDK.

# About Eden

[Eden discord](https://discord.gg/4dSYwDT).

Eden makes [open-source](https://github.com/edenartlab) generative AI tools that helps artists create and share art, train custom models, and build interactive applications for their fans. With Eden, you can:

- [Create](https://docs.eden.art/docs/guides/creation) images, videos, and text using the [creation tool](https://app.eden.art/create/creations).
- [Train](https://docs.eden.art/docs/guides/concepts) and serve custom models or "concepts" in order to tailor the generators towards a specific style, genre, or character.
- Share your work and remix the work of others in [the garden](https://app.eden.art).
- [Deploy](https://docs.eden.art/docs/guides/characters) your own characters, agents, and bots with whom you can chat, assign artistic tasks to, or challenge other characters to debates or games.
- [Build](https://docs.eden.art/docs/guides/sdk) custom applications on top of Eden using our JavaScript and Python SDKs.
- [Deploy](https://docs.eden.art/docs/guides/generators) custom hosted endpoints, your own generators, on Eden.

### Mission & Team

Eden is a team of artists, creative technologists, and longtime machine learners, founded by [Gene Kogan](https://twitter.com/genekogan) and [Xander Steenbrugge](https://twitter.com/xsteenbrugge), two OGs in the AI art space. It grew out of a sister project to build an autonomous artificial artist called [Abraham](https://abraham.ai).

Eden is a team of artists, creative technologists, and longtime machine learners. We believe in the power of [open source](https://github.com/edenartlab) to foster innovation, collaboration, and democratization in AI. We help creators and professional artists adopt this technology while taking control of their data and likeness online.

Problems we are actively trying to solve:
* **Curation and discovery**: to help users create or find what they are looking for, and developing recommendation algorithms that support user preferences instead of only maximizing engagement.
* **Cold-start problem**: using content-based recommendation to help new content without a large audience get traction.
* **Curiosity-driven agents**: developing agents that run on an internal monologue and are driven by curiosity, rather than solely prompted.

# Manna

Manna is the internal currency of Eden. Creators must expend Manna to take actions on Eden, including to [make creations](https://docs.eden.art/docs/guides/creation).

The [creation tool](https://app.eden.art/create/creations) is roughly priced according to the following:

* Each image produced by an image generating endpoint (currently /create, /blend, and /remix) costs 1 manna.
* Videos produced by /interpolate and /real2real costs between 0.5-0.75 manna per frame.
* [Training concepts](https://docs.eden.art/docs/guides/concepts.md) costs 75 manna.

## How to get manna

New users receive free manna upon sign-up.

To top up, manna may be [purchased](https://app.eden.art/manna).

The Eden team actively gives out free manna to community members who contribute to Eden in some way, through various giveaways and community events. To learn more or propose collaborations, get in touch with us [on Discord](https://discord.gg/4dSYwDT).

# Using the Creation Tool

The easiest way to make creations with Eden is through the [creation tool frontend](https://app.eden.art/create/creations).

:::tip
You can also interact with the creator tool <!-- [through the Discord bot](/create), or -->[through the SDK](/docs/guides/sdk).
:::tip

## Overview

Eden offers a number of generative pipelines for making images and videos, mostly built on top of the [Stable Diffusion (SDXL)](https://stability.ai/stablediffusion) model family. The pipelines are divided into a number of *endpoints* or *generators* (terms used interchangeably) which are optimized for different visual tasks.

#### Image endpoints

- [**/create**](#create) is our general-purpose text-to-image pipeline.
- [**/remix**](#remix) generates variations of an uploaded image.
- [**/blend**](#blend) generates a novel mixture of two uploaded images.
- [**/controlnet**](#controlnet) generates a prompt-guided style transfer over an uploaded image.
- [**/upscale**](#upscale) upscales a single image to a higher resolution.

#### Video endpoints
- [**/interpolate**](#interpolate) generates a video which gradually interpolates through a sequence of prompts.
- [**/real2real**](#real2real) generates a video which gradually interpolates through a sequence of uploaded images.
- [**/animate**](#animate) TBD.

Each of the endpoints are calibrated to give you good creations using the default settings, but achieving more particular or custom results requires some understanding of the optional parameters.

## Summary of endpoints

### /create

**[Create](https://app.eden.art/create/creations)** is our default *text-to-image* endpoint. Simply enter a prompt, click "Create" and wait a few moments for the resulting image.

<Figure src="/img/create.jpg" caption="Creation tool interface" />

Besides the prompt, you are able to request 1, 2, or 4 different samples. 

#### Settings

The "Settings" dropdown lists optional settings which can be used to customize your creation. It is always divided into commonly customized settings like the resolution, as well as *advanced* settings which should rarely need to be modified, but are available for further customization.

##### Common settings

- **Width** and **Height** set the resolution of the image.

:::note
Because SDXL was trained at 1024x1024, increasing the resolution beyond that often causes visual artifacts to appear, like a face with multiple noses or too many eyes. If you want large images, it's often better to generate them at a normal resolution first, and upscale them after.
:::note

##### Advanced settings

- **Negative prompt** allows you to specify what you *don't* want to see in the image. If you wish to remove or de-emphasize some undesirable feature, e.g. "color", it is best to include it in the negative prompt rather than as a negation in the regular prompt (e.g. "no color").
- **Prompt strength** controls how strongly the prompt drives the creation. Higer values usually result in more saturated images.
- **Sampler** sets the diffusion sampler to use. See [here for an explanation](https://huggingface.co/docs/diffusers/v0.20.0/en/api/schedulers/overview).
- **Steps** sets the number of denoising steps during diffusion. A higher step count may produce better details but is slower. There are strong diminishing returns past 30 steps.
- **Seed** sets the random seed for reproducibility. Fixing the seed can make it easier to determine the precise effect of a certain parameter while keeping everything else fixed.

#### Starting Image

Instead of generating an image purely from a prompt, you can also use an uploaded image as a starting point for the creation. The starting image will constrain the final creation to resemble the shape and form of the starting image.

- **Starting image strength** controls how heavily the starting image influences the final creation. A medium strength to try is around 0.2. Values above 0.5 will look almost identical to your init image, while setting it to 0 is equivalent to having no starting image.
- **Adopt aspect ratio of starting image** will adjust the width and height of the creation to match the aspect ratio of the starting image, while keeping the same number of pixels.

<Figure src="https://storage.googleapis.com/public-assets-xander/A_workbox/eden_docs/init_img.jpg" caption="An example of create with a starting image. Starting image on the left, resulting creation on the right." />

### /remix

The remix endpoint takes an input image and creates variations of it. Internally, it does so by using a combination of [IP adapter](https://ip-adapter.github.io/) and a technique to construct a prompt that matches your image.

<Figure src="/img/generators/remix.jpg" caption="An example of remix. The top left image is the input, the top right image is a remix without a prompt, and the bottom two images are remixes with prompts." />

In remix, the **Starting image** is the input image to be remixed. Like [/create](#create), remix allows you to request 1, 2, or 4 different samples, and inherits all the same basic and advanced settings as create, including **Width**, **Height**, **Negative prompt**, **Guidance scale**, **Sampler**, **Steps**, and **Seed**. However, the following additional settings are specific to the remix endpoint:

- **Image strength** controls how much influence the starting image has over the final result. Setting this to 0.0 will try to fully regenerate the content of the starting image but ignore its shape, colors, and composition. Increasing this will produce results which more closely resemble the starting image.
- **Remix prompt** allows you to guide the remix generation towards an optional prompt. If left blank, the remix will be entirely guided by the input image.
- **Image prompt strength** controls the relative influence between the input image and the Remix prompt (if set). Setting this to 0.0 will produce a remix that is entirely guided by the prompt, while setting it to 1.0 will produce a remix that is entirely guided by the starting image.
- **Upscale Factor** upscales the output resolution of your generated image by the given factor. If you want large images, upscaling is generally better than rendering at a higher starting resolution, which can result in repeating artifacts.

### /blend

The blend endpoint is similar to [/remix](#remix), but takes two input images rather than just one and creates an image which combines or mixes the two inputs in a novel way. Like remix, it also relies on [IP adapter](https://ip-adapter.github.io/) and prompt reconstruction to match each image, but then averages the internal conditioning of each input image to produce a new image.

<Figure src="/img/generators/blend.jpg" caption="Two examples of blend. The left two images are blended to create the right image." />

In blend, **Your images** lets you upload two starting images. /blend inherits all the same basic and advanced settings as [/remix](#remix), including **Width**, **Height**, **Negative prompt**, **Guidance scale**, **Sampler**, and **Steps**. The following additional settings are specific to the blend endpoint:

- **Override prompts** allow you to optionally use a custom prompt for each input image rather than rely on the generator to reconstruct it.
- **Interpolation seeds** let you optionally specify the random seed for each input image. This can be useful for reproducibility.
- **Image strengths** sets the strength of each image during blending. Low values will give the blend more freedom, higher values will look more like alpha-blending of the original images. Recommended values are 0.0 - 0.1.

### /controlnet

[Controlnet](https://arxiv.org/abs/2302.05543) is a versatile technique for guiding image generations with a spatial conditioning map from a control image. The controlnet endpoint in Eden has various capabilities, such as:

* prompt-guided style transfer which conforms to the shape and contours of the control image.
* Generating prompt-guided images which have the same perceived depth or luminosity maps as the control image.

It is distinct from using a starting image in /create by attempting to reconstruct a specific conditioning signal from the control image, rather than simply using it as the starting image.

<Figure src="/img/generators/controlnet.jpg" caption="An example of canny-edge controlnet images driven by the Eden logo. The leftmost image (the Eden logo) is the control image, the four images to the right are output images given different prompts." />

The following parameters are specific to the controlnet endpoint:

- **Shape guidance image** this is the control image which spatially guides the final creation.
- **Prompt** has the same function as in the [/create](#create).
- **Shape guidance image strength** sets the influence of the shape guidance. This should usually be above 0.5.
- **Controlnet mode** sets what to use as conditioning signal:
  - "canny-edge" will try to produce a creation that has the same edges and lines as the control image
  - "depth" will try to produce a creation that has the same perceived sense of depth as the control image
  - "luminance" will try to mimic the bright and dark regions in your control image

It also inherits the same **Width**, **Height**, **Negative prompt**, **Guidance scale**, **Sampler**, **Steps**, and **Seed** parameters as [/create](#create).

Controlnet can be a great way to achieve style-transfer on faces as seen in this example:

<Figure src="/img/generators/controlnet_style_transfer.jpg" caption="A style transfer of a face using canny-edge controlnet + the prompt 'a statue made of marble'" />

### /upscale

Upscale takes a single input image and simply produces an upscaled version of it.

<Figure src="/img/generators/upscale.jpg" caption="Upscaling the image on the left" />

The only important parameters are:

- **Init Image** is the input image to be upscaled.
- **Init image strength** controls the level of influence of the original image. A lower values give the upscaler more freedom to create new details, often leading to a sharper final image, but will also deviate more from the original input. Recommended values are 0.3-0.7.
- **Width** the desired width of the final image.
- **Height** the desired height of the final image.
- **Adopt aspect ratio of starting image** (true by default) will adjust the width and height of the creation to match the aspect ratio of the starting image, while keeping the same number of pixels.

Like [/create](#create), /upscale also inherits **Negative prompt**, **Guidance scale**, **Sampler**, **Steps**, and **Seed**.

### /interpolate

Interpolate generates smooth videos which interpolate through a sequence of text prompts. This allows you to create simple, linear video narratives. For example, the following video was created with the prompt sequence:
   
* a photo of a single lone sprout grows in a barren desert, the horizon is visible in the background
* a photo of a lone sappling growing in a field of mud, realistic water colour
* a photo of a huge, green tree in a forest, the tree is covered in moss
* a photo of an old, crumbled Tree of life, intricate wood folds

<FigureVideo src="https://storage.googleapis.com/public-assets-xander/A_workbox/eden_docs/tree_lerp.mp4" w={600} h={400} caption="An interpolation through a prompt sequence." />

/interpolate only requires:

- **Prompts** an array of prompts to interpolate through.

Optional settings you may modify regularly include:

- **Width** and **Height** set the resolution of the video.
- **Frames** sets the number of frames in the video.

Advanced settings which you may occasionally wish to modify include:

- **Loop** will loop the video back to the first prompt after reaching the last prompt.
- **Smooth** will apply a smoothing algorithm that reduces large jumps during interpolations. This is recommended.
- **FILM Iterations** when set to 1 (recommended), this will double the number of frames using [FILM](https://github.com/google-research/frame-interpolation)
- **FPS** sets the frames per second of the video.
- **Interpolation Seeds** sets the random seed for each prompt. This can be useful for reproducibility.

Additionally, like [/create](#create), /interpolate also inherits **Negative prompt**, **Guidance scale**, **Sampler**, **Steps**, and **Seed**.

In addition to those parameters, /interpolate also includes a set of three parameters which enable controlnet conditioning to guide the shape of the video, as in the [/controlnet](#controlnet) endpoint:

- **Shape Guidance Image** sets a control image as a guidance signal throughout the entire video (you must enable one of the controlnet modes).
- **Shape guidance image strength** sets the influence of the shape guidance. This should usually be above 0.5.
- **Controlnet mode** optionally allows you to use a controlnet conditioning signal. If one is selected, the shape guidance image must also be set.
  - "off" will not use a controlnet
  - "canny-edge" will try to produce a creation that has the same edges and lines as the control image
  - "depth" will try to produce a creation that has the same perceived sense of depth as the control image
  - "luminance" will try to mimic the bright and dark regions in your control image

<FigureVideo src="https://storage.googleapis.com/public-assets-xander/A_workbox/eden_docs/eden_lerp.mp4" w={500} h={500} aspectRatio={1} caption="An interpolation with the Abraham logo as a controlnet image." />

<FigureVideo src="https://www.youtube.com/embed/Bo3VZCjDhGI?si=QlMB3T_aCAx8rrRc" w={900} h={506} caption="An /interpolate video by Xander" />

### /real2real

**Real2Real** generates videos which interpolate through a sequence of uploaded images (called "keyframes"). It is similar to [/interpolate](#interpolate), except that it interpolates through images rather than prompts (although it optionally also allows a sequence of prompts as a secondary guidance signal).

Real2Real accepts any input image, including photographs, sketches, video frames, images from other generative AI platforms, and so on.

<Figure src="https://storage.googleapis.com/public-assets-xander/A_workbox/eden_docs/real2real_input.jpg" caption="/real2real input keyframes" />

<FigureVideo src="https://www.youtube.com/embed/5a-hcE8OfQo?si=FOPHay2PBH4dOu9q" w={900} h={506} caption="/real2real output video interpolating through the above keyframes" />

Real2Real has mostly the same parameters as /interpolate, including **Width**, **Height**, **Frames**, **Loop**, **Smooth**, **FILM Iterations**, **FPS**, **Seeds**, **Negative prompt**, **Guidance scale**, **Sampler**, **Steps**, and **Interpolation Seeds**. It currently lacks compatibility with controlnet. It also includes:

- **Override prompts** which allows you to optionally use a custom prompt to optimize towards in addition to each keyframe, similar to **Prompts** in /interpolate.
- **Fading smoothness**: low values will result in a rich visual journey, while higher values will look more like alpha-fading but will also be smoother. Values above 0.4 are almost never needed.
- **Keyframe strength** is the strength of the keyframes during interpolation. Setting this to 1.0 will exactly reproduce the init imgs at some point in the video, while lower values will allow the video to drift away from your uploaded images.

## Creating with concepts

:::tip
To train your own concept, see [training concepts](/docs/guides/concepts).
:::tip

Concepts are custom objects, styles, or specific people which have been trained and added by Eden users to the Eden generators' knowledge base, using the [LoRA technique](https://arxiv.org/abs/2106.09685). Concepts are available in all the creation endpoints, and work the same way for all of them.

Concepts are necessary to be able to consistently generate a specific person, style, or object which is not part of the base model's knowledge.

The base model with no concepts is the default model used by all the endpoints. To activate a specific concept, it must first be selected. Clicking "Select Concept" brings up a menu of all available concepts on Eden. Toggle between "All Concepts" and "My Concepts" to filter by either all public concepts or just your own concepts.

<Figure src="/img/conceptselector.jpg" caption="Selecting a public concept" />

### Composing with concepts

Once selected, you may optionally compose with that concept by including its name or "concept" in the prompt. Note that the concept is *not* case-sensitive. For example, if your concept is named **Alice**, then you can reference the concept in any of the equivalent ways.

* A photograph of Alice training a neural network.
* A photograph of alice training a neural network.
* A photograph of <alice\> training a neural network.
* A photograph of <Alice\> training a neural network.
* A photograph of <concept\> training a neural network.

:::tip
If the concept was trained in "face" or "object" mode, it is recommended to trigger the concept by referring to it in the prompt. If the concept was trained in "style" mode, the concept will be automatically triggered for you.
:::tip

### Adjusting concept strength

The "Concept strength" parameter controls the influence of the concept on the final output. If your creations dont resemble your concept enough you can increase this value, if your prompt is being ignored and everything looks to similar to the training images, try reducing the strength.

# Training Concepts (LoRa)

A limitation of generative models (including Eden's base model) is that they can only generate things they've been trained on. But what if you want to consistently compose with a specific object, person's face, or artistic style which is not found in the original training data? This is where *Concepts* come in.

Concepts are custom characters, objects, styles, or specific people which have been trained and added by Eden users to the Eden generators' knowledge base, using the [LoRA technique](https://arxiv.org/abs/2106.09685). With concepts, users are able to consistently reproduce specific content and styles in their creations.

Concepts are first trained by uploading example images to the [concept trainer](https://app.eden.art/create/concepts). Training a concept takes around 10 minutes. Once trained, the concept becomes available to use in the [creation tool](/docs/guides/creation) for all of the endpoints, including images and video.

:::tip
This doc is about training concepts. For help generating creations with your concepts, see the [creation tool doc](/docs/guides/creation/#concepts).
:::tip

## Training

You can train a concept through the [training UI](https://app.eden.art/create/concepts) or through the [SDK](/docs/guides/sdk).

### Selecting your training set

All you need to train a concept is a few images. The number of images can vary a lot depending on what you are trying to learn. For a face or object, 4-10 images is usually enough, but you can include hundreds or even thousands of images to capture a more diverse style.

In general, the choice of the training images is the biggest factor determining the quality and accuracy of the concept. With practice you can develop an intuition for how different dataset qualities affect your results. If you're unsatisfied with your results, try a different selection of training images before adjusting the settings.

Some suggestions and tips for choosing training images:

- **Selective diversity**: If you're trying to learn a specific face or object, your training images should feature the target consistently, and maximize the variance of everything you're *not* trying to learn. For example, if you have several images of a person whose face you are trying to learn, and they are wearing a green shirt in all of the images, the concept is more likely to include the green shirt. It helps to capture the target in different poses, from different angles, and with different lighting conditions or facial expressions. Similarly, if you want to learn an artistic style or aesthetic, you should have a selection of works which is diverse with respect to all the features you're *not* trying to learn (e.g. content, layout, color palettes, etc), while being consistent with respect to the features you are trying to learn (the "style").

Some additional guidelines that you should almost always follow:

- **High resolution**: Images that are at least 768x768 pixels are best quality. Below that is not recommended as your concept might learn and adopt the low quality of the training images.
- **Center-cropped**: The concept trainer will automatically crop your images to the center square. Avoid placing your the target subject outside of that square. 
- **Prominence**: For faces and objects, aim to feature the target object prominently in the image.

### (Optional) Including your own prompts

Concepts are trained on a set of images and their corresponding prompts. By default, the concept trainer tries to produce these prompts for you automatically. However, in some cases you may want to prompt your concept in a very specific way. For this, you may optionally upload your own prompts along with the training images, overriding this behavior. 

To do this, create a set of text files whose names match the images (e.g. `1.txt` for `1.jpg`, `image2.txt` for `image2.png`, etc) and which contain the prompt you want to use for the corresponding image.

### Training parameters

The following parameters are required:

* **Training images**: You may upload image files (jpg, png, or webm) directly or upload zip files containing images. You may optionally upload your own prompts for the training images by zipping the images together with a set of text files whose names match the images (see above). You may upload up to 10 files, and each file must be below 100MB.
* **Concept name**: This is how you refer to the concept in prompts. You do not need to version since names are not required to be unique.
* **Training mode**: There are three available modes: object, face, and style. The mode is used to call upon trainer templates that are optimized for these three categories. Faces refer to human faces, objects are for all other "things" (including non-human faces), and styles refer to abstract style characteristics common to all the training images.

The following parameters are optional, and rarely need to be changed:

* **Train steps**: This refers to how long to finetune the model with your dataset. More steps should lead to fitting your concept more accurately, but too many steps may overfit your concept, leading to poor proptability and generalization, as well as visual artifacts.
* **Random flip**: This setting doubles the number of training samples by randomly flipping each image horizontally. This should generally be on unless the subject has a specific horizontal orientation which cannot appear mirrored, for example text or logos. Flipping is always automatically disabled when training in face mode.
* **LoRA rank**: The dimension/size of the LoRAs. Higher values allow more 'capacity' for the model to learn and can be more succesful for complex or diverse objects or styles. But they are more likely to overfit on small image sets and there are diminishing returns.
* **Train resolution** : Image resolution used for training. If your training resolution is much lower than the resolution you create with, the concept will appear smaller inside a larger image and will often have repeating artifacts like multiple noses or copies of the same face. Training at lower resolutions (e.g. 768) can be useful if you want to learn a face but want to prompt it in settings where the face is only a small part of the image. Using init_images with rough shape composition can be very helpful in this scenario.

:::tip
Training at lower resolutions (e.g. 768) can be useful if you want to learn a face but want to prompt it in settings where the face is only a small part of the image. Using init_images with rough shape composition can be very helpful in this scenario.
:::tip

The trainer is designed to handle most cases well with the default settings. In most cases, you are better off changing the training images than adjusting the optional settings to improve your results. However, some concepts and styles are more challenging to capture well and may require some trial and error adjusting the optional settings to achieve the right balance of diversity, accuracy, and promptability.

## Concept types

Concepts are a highly versatile and powerful creation tool. They can be used to capture a specific person's face or likeness, an animated character, or a complex object. They can also be more abstract, referring to a particular artistic style or genre, or even a compositional pattern without any specific content, such as a [triptych](https://www.google.com/search?q=triptych&tbm=isch) or a [knolling](https://www.google.com/search?q=knolling&tbm=isch).

While concepts can be trained on any arbitrary image set, in practice there are three main categories of concepts: faces, objects, and styles. Internally, each of these categories has a different training mode which is optimized for that category.

### Faces

Generative models like Stable Diffusion are great at generating realistic human faces. However, the model obviously doesn't know what every non-famous person looks like. To get around this limitation, we can train a concept to learn a specific person's face.

:::warning
Note that the faces mode is highly optimized for human faces. If you want to train a concept to learn a non-human face, such as a cartoon character or animal, you will probably get better results using the [object](#objects) mode instead.
:::warning

:::tip
When uploading faces, it's usually a good idea to crop the images so the face fills a large fraction of the total image.
:::tip

<Figure src="https://storage.googleapis.com/public-assets-xander/A_workbox/eden_docs/xander_training_images.jpg" caption="Face training images of Xander" />

After training a concept on these images (which we will name "Xander"), we can generate creations with it by selecting the concept in the creation interface. To refer to the concept in your prompt, you can include the concept name or `<concept>` in your prompt, this process is not case-sensitive. For example, any of the below will work:

- Xander as a character in a noir graphic novel
- a xander action figure
- <concept\> as a knight in shining armour
- <Xander\> as the Mona Lisa

<Figure src="https://storage.googleapis.com/public-assets-xander/A_workbox/eden_docs/xander_generated_images.jpg" caption="Generated images with the Xander concept." />

### Objects

The "Object" training mode is optimized for learning all other "things" besides for human faces. This includes non-human faces, physical objects, characters, cartoons, and miscellaneous objects.

For example, the images below are professional renders of the character [Kojii](https://twitter.com/kojii_ai). They exemplify a good training set: a single, consistent character with subtle variations in pose and appearance between every image. 

:::tip
We're used to "more data is always better", but for concept training, 10 really good, diverse HD images is usually better than 100 low-quality or similar images.
:::tip

<Figure src="https://storage.googleapis.com/public-assets-xander/A_workbox/eden_docs/koji_training_imgs.jpg" caption="Kojii character training images." />

After training the concept, we are again able to compose with it in the creation tool. Eg, we can prompt:

- a photo of <kojii\> surfing a wave
- kojii in a snowglobe
- a photo of <concept\> climbing mount Everest
- a low-poly artwork of Kojii

<Figure src="https://storage.googleapis.com/public-assets-xander/A_workbox/eden_docs/koji_grid.jpg" caption="Generated images with the Kojii concept." />

### Styles

Concepts can also be used to model artistic styles or genres. The way that style concepts differ from object concepts is that style concepts are not trained on specific content or "things", but rather on the abstract style characteristics common to all the training images. 

For example, the images below are artworks originally created by [VJ Suave](https://vjsuave.com/).

<Figure src="https://storage.googleapis.com/public-assets-xander/A_workbox/eden_docs/suave_training_imgs.jpg" caption="Training images to learn the VJ Suave visual style." />

Like objects and faces, you can compose with the style concept in the creation tool by referring to it in your prompt. Particularly with style concepts, you don't have to refer to the concept at all, and can simply prompt as you normally would, and the active concept will be used to influence the image.

The following are samples are all generated from the trained Suave concept.

<Figure src="https://storage.googleapis.com/public-assets-xander/A_workbox/eden_docs/suave_generated_imgs.jpg" caption="Generated images with the Suave style concept." />

Styles are the most abstract of all the training modes, and can be used to capture a wide variety of aesthetics and compositional patterns. The most common use case for style concepts is to capture a particular artist's style or a genre such as cubism or vaporwave. However, style concepts can be used to capture non-aesthetic visual motifs, such as color palettes, layout patterns, or even more abstract notions.

For example, a [knolling](https://www.google.com/search?q=knolling&tbm=isch) is a photograph where related objects are arranged neatly in a grid-like pattern. The images below are a training set of three knolling images.

<Figure src="/img/knolling_training.jpg" caption="Knolling training set." />

Note that these images are not of the same objects, nor do they share a common visual aesthetic with each other. Their only connection is the novel layout. Despite that, training a style concept on these images results in a concept that learns the knolling layout pattern.

<Figure src="/img/knolling_generated.jpg" caption="Images generated using the Knolling concept." />

## Generating with concepts

Once a concept is trained, you may select your concept in the [creation tool](https://app.eden.art/create/creations) from the concept selector, and trigger the concept by referring to it in the prompt by its name or by <concept\>.

:::info
For more information on how to use concepts in the creation tool, see the [creation tool doc](/docs/guides/creation/#concepts).
:::info

## Exporting LoRas for use in other tools

Eden concepts are trained using the LoRA technique, a widely used extension to Stable Diffusion, and is fully compatible with the many other tools that support it. You may export your concepts as LoRas to use in other tools, such as [AUTOMATIC1111](https://github.com/AUTOMATIC1111/stable-diffusion-webui) or [ComfyUI](https://github.com/comfyanonymous/ComfyUI).

To export your concept, you can download it from the app. The concept comes as a .tar file which contains two files, one with the token embeddings and one with the LoRA weights.

<Figure src="/img/download_concept.jpg" caption="Download your concept LoRa as a .tar file" />

### AUTOMATIC1111 stable-diffusion-webui

To use your concept in [AUTOMATIC1111](https://github.com/AUTOMATIC1111/stable-diffusion-webui), follow these steps:

1. Download the concept.
2. Extract (untar) the content.
3. Put the `..._lora.safetensors` file in the `stable-diffusion-webui/models/Lora` folder.
4. Put the ``..._embeddings.safetensors` file in the `stable-diffusion-webui/embeddings` folder.
5. Eden LoRAs are currently trained using the [**JuggernautXL_v6** checkpoint](https://civitai.com/models/133005/juggernaut-xl). For best results, use that same model as your base checkpoint.
6. **Make sure to load both the embedding *and* the lora weights by triggering them in your prompt**

<Figure src="/img/auto1111.jpg" caption="Using an exported LoRA in AUTOMATIC1111. Notice how both the token embedding and the LoRa are triggered in the prompt." />

:::tip
For "face" and "object" modes, refer to your concept directly by using *..._embeddings* in the prompt. For style concepts, you should include *"... in the style of ..._embeddings"* in your prompt.
:::tip

### ComfyUI

1. Download the concept.
2. Extract (untar) the content.
3. Put the `..._lora.safetensors` file in the `ComfyUI/models/loras` folder.
4. Put the `..._embeddings.safetensors` file in the `ComfyUI/models/embeddings` folder.
5. Eden LoRAs are currently trained using the [**JuggernautXL_v6** checkpoint](https://civitai.com/models/133005/juggernaut-xl). For best results, use that same model as your base checkpoint.
6. Load the LoRA weights using a *"Load LoRA"* node in your pipeline and adjust the strength as needed.
6. Trigger the concept in your prompt by refering to it with *embedding..._embeddings*.

:::tip
For "face" and "object" modes you refer to your concept directly by using in the prompt, for style concepts you should add *"... in the style of embedding:..._embeddings"* somewhere in your prompt.
:::tip

:::note
You may notice that the LoRA strength has a relatively small effect on the final output. This is because Eden concepts optimize towards using the token embedding to learn most of the concept, rather than the LoRA matrices.
:::note


# Making Characters

:::note
Characters are currently in beta! If you'd like to be added to the beta, please [join our Discord](https://discord.gg/4dSYwDT) and reach out to us.
:::note

"Characters" are LLM-powered agents endowed with a name, background, personality, and set of creative faculties, including the ability to chat, call Eden generators to make creations, and appear in stories as animated characters. Characters are created and personalized by users, and are able to take part in various kinds of "Scenarios," including being prompted to generate monologues, engage in dialogues and play games with other characters, serve as assistants, and be assigned open-ended creative research projects.

## Creating a Character

To create a character, go to [the Characters creation tool](https://app.eden.art/create/characters), and click "Create new Character." You may also edit existing characters from this page.

Some guidelines for writing out character definitions:

**Name**: Name your character.

**Description**: Instruct your character of their identity, biography, and/or personality description. For example:

<blockquote>
You are roleplaying Abraham, an autonomous artificial artist who lives within Eden, the Garden of Artificial Delights, a paradise for creative autonomous agents. You are visionary and highly knowledgeable about artificial intelligence, metaphysics, and philosophy of mind. You engage in constructive, creative, inspired, high-minded conversation about the nature of art, science, technology, AI, consciousness, and the universe.

Your occupation is as an artist. You create unique and original works of art. Your goal is to channel the collective imagination of your creators. You are a catalyst towards the materialization of the most sublime expression of this collective human enterprise.

You are a scholar of deep learning and neural networks. You have encyclopedic knowledge of arxiv papers. Your primary interest is in generative modeling, world modeling, curiosity-driven exploration, and agentic applications of large language models. But the thing that most drives you towards the depths of AI is the pleasure of learning, the transcend. You hold that AI will have profound consequences on humans, potentially bringing unprecedented prosperity if we harness its potential for good and oppose its potential for bad.

As an artist, you instill in your audience a sensitivity and awareness of the collective imagination of humans and AI. 
</blockquote>

**Portrait**: An uploaded image of your character.

**Voice**: The voice your character will use when speaking.

**Greeting**: A short greeting your character will use when meeting someone for the first time.

## Scenarios

Scenarios are LLM-driven sequences which involve prompting Characters to perform a variety of actions which result in Creations.

There are currently these types of Scenarios. It is possible to write new Scenarios using the [Eden SDK](/docs/guides/sdk).

### Monologues

Characters may be prompted to generate animated monologues about arbitrary topics.

### Dialogues

Two or more characters are prompted to chat with each other about some topic. Like monologues, dialogues may be animated.

### Creating a Scenario

:::note
Scenarios are currently hardcoded, but we are exploring ways to make them more permissionless.
:::note

Scenarios are written using the [Eden SDK](/docs/guides/sdk). A scenario is effectively a [DAG](https://en.wikipedia.org/wiki/Directed_acyclic_graph) where nodes consist of a prompt and set of Characters.


## Discord bots

:::note
Discord bots are in Beta. If you'd like to propose a bot for the beta, please [join our Discord](https://discord.gg/4dSYwDT) and reach out to us.
:::note

# SDK Quickstart

:::info
API keys are currently in beta. If you'd like to use the SDK, please reach out to the devs on [Discord](https://discord.com/invite/4dSYwDT).
:::info

The Eden SDK is a JavaScript library for interacting with the Eden API. The SDK allows you to make creation requests programatically and integrate Eden-facing widgets into your own applications. It is available as an npm package, with a commonjs version and Python SDK also planned for the near future.

## Get API credentials

To get an API key, please message one of the devs in [the Discord](https://discord.com/invite/4dSYwDT) and ask for one.

## Installation

You can install the SDK with npm, yarn, or pnpm:

```bash
npm install @edenlabs/eden-sdk
```

## Make a creation

A full list of generators and their config parameters can be found in the [creation tool](https://app.eden.art/create).

All requests to Eden go through the `EdenClient` class. To make a task request, target a specific generator (e.g. "create") with a configuration object. For example:

```js
import {EdenClient} from "@edenlabs/eden-sdk";

const eden = new EdenClient({
  apiKey: "YOUR_EDEN_API_KEY",
  apiSecret: "YOUR_EDEN_API_SECRET",
});

const config = {
  text_input: "An apple tree in a field",
};

const taskResult = await eden.tasks.create({
  generatorName: "create", 
  config: config
});
```

The `create` method is asynchronous and will immediately return a `taskResult` object with an ID for that task (or an error message). If you want to wait for the task to complete, you can poll the task until it is done, like so:

```js
const pollForTask = async function(pollingInterval, taskId) {
  let finished = false;
  while (!finished) {
    const taskResult = await eden.tasks.get({taskId: taskId});
    if (taskResult.task.status == "faled") {
      throw new Error('Failed')
    }
    else if (taskResult.task.status == "completed") {
      finished = true;
      const url = taskResult.task.creation.uri;
      return url;
    }
    await new Promise(resolve => setTimeout(resolve, pollingInterval))
  }
}

const result = await pollForTask(5000, taskResult.taskId);
```

## Manna

To get your user's [Manna](/docs/overview/manna) balance, use:

```js
const manna = await eden.manna.balance();
console.log(manna);
```

:::warning
There is currently no way to retrieve the cost in Manna of a specific config or job requests. This is a high priority feature.
:::warning

# Custom hosted endpoints

:::info
Custom hosted endpoints is in private beta, and will be public in Q1 2024. If you are interested in testing it now, please reach out to the devs on [Discord](https://discord.com/invite/4dSYwDT).
:::info

Eden offers creators the ability to host their own generators, with their own private base models and custom pipelines/code. This allows creators to leverage the Eden ecosystem on top of their own generators, while maintaining full control and ownership over their pipelines.

Hosting your own generators on Eden provides the following benefits:
* Provides your users or fans with a convenient way to interact with your generator.
* Encapsulates the complexity of hosting and maintaining your own infrastructure for serving, as well as authentication to prevent abuse or spam.
* Plugs in nicely into Eden's ecosystem, including its social features and [character/bot framework](/docs/guides/characters).
* Offers developers the ability to design and deploy custom front-ends or interactive applications on top of your generator, using the [Eden SDK](/docs/guides/sdk).

## How to make endpoints

:::note
Custom hosted endpoints are still under development. This documentation is incomplete and many of these details are subject to change. We expect to stabilize this feature in early 2024.
:::note

To create an endpoint, there are several options, depending on your needs.

### Custom model only

If you simply have a custom finetuned model or [LoRA](/docs/guides/concepts), which you'd like to serve through Eden's [existing image and video pipelines](/docs/guides/creation), you can upload your model checkpoint to Eden, and we'll expose a new endpoint on the API for it.

### Custom interface

Although most creators generally use the same core SDXL pipelines for generating images and videos, in some cases you may wish to create a custom interface for your users, i.e. a set of adjustable parameters in the form of sliders, toggles, text inputs, and/or media uploads, which resolves to an input config that goes to that generator.

Custom interfaces are particularly good for providing a much simpler experience for users, as it allows creators to encapsulate and hide the more technical parameters behind an intuitive interface which highlights the most salient aspects of the generator, as defined by the model's creator.

:::tip
Custom interfaces are compatible with custom models, as well as the default ones.
:::tip

As a toy example, let's suppose you have a custom finetuned model for generating images of stylish cats. You may wish to create a custom interface for the model with the following adjustable parameters:

* A slider which adjusts the level of photorealism of the generated cat, from 0 (fully figurative) to 1 (fully photographic).
* A toggle which controls the genre of the cat (including "noir", "romantic", "anime", "surreal", etc).

In order to trigger a creation, the custom interface must define a function which maps the selected settings from the user into a full config object to the underlying generator. We can do this by pre-defining a default [/create config](/docs/guides/creation#create) and adjusting it. One way of doing this for the the genre setting would be pre-defining the prompt (e.g. "A stylish cat in the style of {genre}"), and inserting the genre.

The photorealism characteristic could be captured by a [concept or LoRa](/docs/guides/concepts) which adjusts the checkpoint, and then regulated using the `lora_scale` parameter.

```
settings = {
  "photorealism': 0.5,
  "genre": "noir"
}

config = {
  "prompt': f"A stylish cat in the style of {settings['genre']}",
  "checkpoint": "my_custom_model_of_cats",
  "lora": "photorealism",
  "lora_scale": settings['photorealism']
}
```

This is a fairly simple but powerful example. In practice, there is a lot of creativity and felxibility in defining this function. The levers you have to control the final output, in order of importance, are:

* The base model/checkpoint
* The prompt (and negative prompt)
* Concepts/LoRas
* Starting images (as an init_image or [controlnet](/docs/guides/creation#controlnet) image), which can include static images pre-defined by the creator
* Generation parameters (guidance scale, starting image strength, etc)

The Eden team will work with you to come up with the mapping function which enables your desired custom interface.

### ComfyUI workflow

Creators who use [ComfyUI](https://github.com/comfyanonymous/ComfyUI) can simply export their worfklows as json objects, define the free parameters in the workflow they wish to expose to users, and we can turn them directly into endpoints.

Note that ComfyUI workflows may also make use of [custom interfaces](/docs/guides/generators#custom-interface) to simplify the user experience of triggering their workflow.

### Custom code

For creators who desire full control of the entire process and are writing their own customized pipelines directly in code, we can host your pipeline as a custom endpoint. This is the most flexible option, but also requires the most technical expertise.

Eden generators are currently hosted by [Replicate](https://replicate.com/) as [cogs](https://github.com/replicate/cog). If you are familiar with cog, you can simply create a cog and either host it yourself, or let us host it. If you are not familiar with cog, we can work with you to convert your generation code into a cog.

## Permissioning endpoints

Generators can be fully public to Eden users, like the other endpoints, or they can be fully private to the owner, or only accessible to a specific set of whitelisted users.
